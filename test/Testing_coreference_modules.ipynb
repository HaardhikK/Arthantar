{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqfaYn+P0no3yA1z9ViSIg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaardhikK/Arthantar/blob/main/test/Testing_coreference_modules.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Maverik Coref"
      ],
      "metadata": {
        "id": "MXfaPAVSnHRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install maverick-coref"
      ],
      "metadata": {
        "collapsed": true,
        "id": "F4TCO8EhNMae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install the package (run in your terminal if not already installed)\n",
        "# pip install maverick-coref\n",
        "\n",
        "# Step 2: Import Maverick and load the pretrained model\n",
        "from maverick import Maverick\n",
        "\n",
        "# Load the model (choose the desired model)\n",
        "model = Maverick(\n",
        "    hf_name_or_path=\"sapienzanlp/maverick-mes-ontonotes\",  # Model trained on OntoNotes dataset\n",
        "    device=\"cpu\"  # Use \"cuda:0\" for GPU\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690,
          "referenced_widgets": [
            "aa7fea777ca54fa39ca3e8b6f193a3bc",
            "441bc92a79724d948424d08521cbffea",
            "d64812b4b9da40e7b48eb35ff8976ae7",
            "757e302cce3040ad8d6a41bbf8272934",
            "c8d69b37f174401199e8bb1ab21bc9eb",
            "056fd242958747c48661e125ba8f4bb1",
            "13f8d0d06e2f44ad9e155f9be1888bba",
            "fcd80ecb67b54e7da114192a7bffa273",
            "90f142f0e82e47bb9d73dca6bff9c828",
            "673b55732b58416fa1d43505b0942ebd",
            "358da26efe5f4cf08bfd2310735d83d1",
            "20358e78c2df442f85572cffbfb7a3c3",
            "5a7b8d6fd778405190bef1ba3e5efecd",
            "b2b39dc4a62343308ee723e56c9018f3",
            "dea82f1b060746c5a6a0c2e87e672c5a",
            "ef458d535d9b42c594d0611a30716816",
            "762067648b194a719b3c2400a7f51fad",
            "3520bacc77a348099eb64db4e4db88f8",
            "d748ba0c2b4240898749c383c6631204",
            "1f507bac159f4688b19376747a6d64e1",
            "f1c2876eb7874c3b8436b0d259442879",
            "2845594e4354453fb1cb8867e926c9c7",
            "041d80143ce040ff8ae83ac5230edcfa",
            "23a725d148384b1ebf4f7f8be39afa2c",
            "0fb0183bece246ec9462141cfcfc0d56",
            "548481d400eb4a5db4526d86486dbe1a",
            "f5ae2e385ac74bdf95fe390a584e9b2f",
            "9bc8f5a1def94d559120598120460f0f",
            "647ad4a944904fb6befcc4890130b9ab",
            "f20f9c235838413a832639248c804ea2",
            "27edefb9111048359d8d8049378e8cd6",
            "b785bf81b9da4fc18bdac425a70dc80b",
            "65b4791ecd914aa698213cb6780477c0",
            "a0a268257e864481a8f75d686cdda61b",
            "0a878336bd7f4df58e1adb35006c5ab1",
            "841a7e63f7514d3f8d7828b2be404bce",
            "345cfbebd7dd4faf81d7a2cc9c92613d",
            "bee7f9babf0c4f0a81d86c1994b6543d",
            "907c539997314d7286e7b56762789445",
            "b503f1a8ef52456aa0b0b6693e74a2ae",
            "735391d02dce4de2bd75ee1f1b25cbb2",
            "8fc47470cb774dd8bccd50b203f305cc",
            "4de0dd7ab2fd4be5a4419d6385ab1e40",
            "1be4fba3dcce43a19462ca2054fcbf8c",
            "5eaef6ecf3f64ff4a74fc7f8cb61c482",
            "6650463a440545a8ae041d06da93e4cd",
            "8b4de5bc29534d4988dd3fc73e7e11f7",
            "21b499599a9d43b4837b7ce4c122d831",
            "88095150f5034a8493bb958b8bf60a32",
            "ac7c3c64dbac4edbb4d97e677e0ceba0",
            "510f32ab9ac44d499f230629e522d356",
            "d31142c88a5e443a83e6504952be053b",
            "67a996c60a054137aa8ff1ac23f81621",
            "33b1a4c1acee49b5aaf92fdcb7c2b29e",
            "3d24418fd59542beb879acc546d8fbee"
          ]
        },
        "collapsed": true,
        "id": "yPsig6vS_E4b",
        "outputId": "5c72fe58-951c-47d7-f557-ecfe2a35d037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sapienzanlp/maverick-mes-ontonotes loading\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "weights.ckpt:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa7fea777ca54fa39ca3e8b6f193a3bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20358e78c2df442f85572cffbfb7a3c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "041d80143ce040ff8ae83ac5230edcfa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0a268257e864481a8f75d686cdda61b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a878336bd7f4df58e1adb35006c5ab1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-bf0efc10619b>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the model (choose the desired model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m model = Maverick(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mhf_name_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sapienzanlp/maverick-mes-ontonotes\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Model trained on OntoNotes dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m  \u001b[0;31m# Use \"cuda:0\" for GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/maverick/models/maverick_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hf_name_or_path, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_model_tokenizer__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_model_path__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/maverick/models/maverick_model.py\u001b[0m in \u001b[0;36m__get_model_tokenizer__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_model_tokenizer__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_hf_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mspecial_tokens_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"additional_special_tokens\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"[SPEAKER_START]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[SPEAKER_END]\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecial_tokens_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2030\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2032\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   2033\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2034\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2270\u001b[0m         \u001b[0;31m# Instantiate the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2272\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2273\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mimport_protobuf_decode_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2274\u001b[0m             logger.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, tokenizer_file, do_lower_case, split_by_punct, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     ) -> None:\n\u001b[0;32m--> 103\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mtokenizer_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mslow_tokenizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# We need to convert a slow tokenizer to build the backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mfast_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_slow_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslow_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgguf_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# We need to convert a slow tokenizer to build the backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py\u001b[0m in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSLOW_TO_FAST_CONVERTERS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_tiktoken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0mconverter_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSLOW_TO_FAST_CONVERTERS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer_class_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconverter_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# from .utils import sentencepiece_model_pb2 as model_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0mmodel_pb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_protobuf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py\u001b[0m in \u001b[0;36mimport_protobuf\u001b[0;34m(error_message)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimport_protobuf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_sentencepiece_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0msentencepiece\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentencepiece_model_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msentencepiece_model_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/sentencepiece_model_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mcreate_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal_create_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   values=[\n\u001b[0;32m---> 34\u001b[0;31m     _descriptor.EnumValueDescriptor(\n\u001b[0m\u001b[1;32m     35\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UNIGRAM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mserialized_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/protobuf/descriptor.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, name, index, number, type, options, serialized_options, create_key)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_USE_C_DESCRIPTORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m     \u001b[0m_C_DESCRIPTOR_CLASS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOneofDescriptor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m     def __new__(\n",
            "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define the input\n",
        "# ontonotes_format = [\n",
        "#     [\"Kiran\", \"is\", \"a\", \"good\", \"student\", \"and\", \"he\", \"goes\", \"to\", \"school\", \"where\", \"Sarita\", \"is\", \"the\", \"teacher\", \".\"],\n",
        "#     [\"She\", \"teaches\", \"maths\", \".\"]\n",
        "# ]"
      ],
      "metadata": {
        "id": "Al4css5HO6Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ontonotes_format = [\n",
        "    [\"Kiran\", \"is\", \"a\", \"good\", \"student\", \"where\", \"Krishna\", \"is\", \"the\", \"teacher\", \".\"],\n",
        "    [\"She\", \"teaches\", \"maths\", \"and\", \"he\", \"goes\", \"to\", \"school\", \"regularly\", \".\"]\n",
        "]\n"
      ],
      "metadata": {
        "id": "STlleIaR1rSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Perform coreference resolution\n",
        "result = model.predict(ontonotes_format)\n",
        "\n",
        "# Step 5: Display the results\n",
        "print(\"Tokens:\", result['tokens'])\n",
        "print(\"Clusters (Token Offsets):\", result['clusters_token_offsets'])\n",
        "# print(\"Clusters (Text Mentions):\", result['clusters_text_mentions'])\n",
        "\n",
        "# Step 4: Extract and display the results\n",
        "tokens = result['tokens']\n",
        "clusters_token_offsets = result['clusters_token_offsets']\n",
        "\n",
        "# Step 5: Resolve mentions manually\n",
        "clusters_text_mentions = []\n",
        "for cluster in clusters_token_offsets:\n",
        "    mentions = [\" \".join(tokens[start:end + 1]) for start, end in cluster]\n",
        "    clusters_text_mentions.append(mentions)\n",
        "\n",
        "print(\"Clusters (Text Mentions):\", clusters_text_mentions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oQXzdLMNNpx",
        "outputId": "feb859af-e18a-4d90-e735-ddc898c4da1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Kiran', 'is', 'a', 'good', 'student', 'and', 'he', 'goes', 'to', 'school', 'where', 'Sarita', 'is', 'the', 'teacher', '.', 'She', 'teaches', 'maths', '.']\n",
            "Clusters (Token Offsets): [((0, 0), (6, 6)), ((11, 11), (16, 16))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ontonotes_format = [\n",
        "    [\"John\", \"went\", \"to\", \"the\", \"store\", \"with\", \"his\", \"brother\", \"Michael\", \",\", \"who\", \"was\", \"buying\", \"groceries\", \".\"],\n",
        "    [\"Afterward\", \",\", \"they\", \"both\", \"decided\", \"to\", \"grab\", \"a\", \"coffee\", \",\", \"and\", \"it\", \"was\", \"John's\", \"idea\", \"to\", \"go\", \"to\", \"that\", \"particular\", \"café\", \".\"]\n",
        "]\n"
      ],
      "metadata": {
        "id": "dy_zc6i3N8LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Display the results\n",
        "print(\"Tokens:\", result['tokens'])\n",
        "print(\"Clusters (Token Offsets):\", result['clusters_token_offsets'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItMKP0zsUJLj",
        "outputId": "0ba1bd8b-adec-4bd3-b33a-0d824bccc27c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['John', 'went', 'to', 'the', 'store', 'with', 'his', 'brother', 'Michael', ',', 'who', 'was', 'buying', 'groceries', '.', 'Afterward', ',', 'they', 'both', 'decided', 'to', 'grab', 'a', 'coffee', ',', 'and', 'it', 'was', \"John's\", 'idea', 'to', 'go', 'to', 'that', 'particular', 'café', '.']\n",
            "Clusters (Token Offsets): [((0, 0), (6, 6), (28, 28))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Perform coreference resolution\n",
        "result = model.predict(ontonotes_format)\n",
        "\n",
        "\n",
        "# Step 4: Extract and display the results\n",
        "tokens = result['tokens']\n",
        "clusters_token_offsets = result['clusters_token_offsets']\n",
        "\n",
        "# Step 5: Resolve mentions manually\n",
        "clusters_text_mentions = []\n",
        "for cluster in clusters_token_offsets:\n",
        "    mentions = [\" \".join(tokens[start:end + 1]) for start, end in cluster]\n",
        "    clusters_text_mentions.append(mentions)\n",
        "\n",
        "print(\"Clusters (Text Mentions):\", clusters_text_mentions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vaAx95-Tj1N",
        "outputId": "35a1655c-db0b-471e-a6aa-9f1e40166c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clusters (Text Mentions): [['John', 'his', \"John's\"]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ontonotes_format = [\n",
        "    [\"John\", \"went\", \"to\", \"the\", \"store\", \"with\", \"his\", \"brother\", \"Michael\", \",\", \"who\", \"was\", \"buying\", \"groceries\", \".\",\"Afterward\", \",\", \"they\", \"both\", \"decided\", \"to\", \"grab\", \"coffee\", \",\", \"and\", \"John\", \"suggested\", \"going\", \"to\", \"the\", \"café\", \".\"]\n",
        "]\n"
      ],
      "metadata": {
        "id": "yRBhF26kT1MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Perform coreference resolution\n",
        "result = model.predict(ontonotes_format)\n",
        "\n",
        "\n",
        "# Step 4: Extract and display the results\n",
        "tokens = result['tokens']\n",
        "clusters_token_offsets = result['clusters_token_offsets']\n",
        "\n",
        "# Step 5: Resolve mentions manually\n",
        "clusters_text_mentions = []\n",
        "for cluster in clusters_token_offsets:\n",
        "    mentions = [\" \".join(tokens[start:end + 1]) for start, end in cluster]\n",
        "    clusters_text_mentions.append(mentions)\n",
        "\n",
        "print(\"Clusters (Text Mentions):\", clusters_text_mentions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGHQW-RoUkOw",
        "outputId": "91d2e6f1-5220-449d-ed59-4774122f57f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clusters (Text Mentions): [['John', 'his', 'John']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lingmess Coref {tested in different collab isnt that good}"
      ],
      "metadata": {
        "id": "a_f_IOfrpafD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fast Coref"
      ],
      "metadata": {
        "id": "qfySyuufnNJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "pip install fastcoref"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OC-wJzBBUk9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastcoref import FCoref\n",
        "\n",
        "# Initialize the model with GPU (ensure you're using CUDA device)\n",
        "model = FCoref()"
      ],
      "metadata": {
        "id": "ifmnWBYSn48R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict coreference resolution for your input text\n",
        "preds = model.predict(\n",
        "    texts=['Kiran is a good student and he goes to school where Sarita is the teacher. She teaches maths.']\n",
        ")\n",
        "\n",
        "# Retrieve the coreference clusters from the prediction\n",
        "clusters = preds[0].get_clusters()\n",
        "\n",
        "# Print the clusters\n",
        "print(\"Coreference Clusters:\", clusters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "b9bb1762c32c4b2f9a2242ba90671360",
            "8853a451bbd747f1afc97aaf0f03ade6",
            "ffc4a472d0ea4e8188f8aa8fbc34bd8d",
            "03ad2f43422c4f9e8e2547b556e37f9a",
            "7bf7608f80294d6b83a703cfea1653d4",
            "770a62c0641b43199cc607905e69bbd1",
            "d5ea2e483e6d4dc2bbd536b9a3c370e5",
            "b1e153359693462ba422af88bf812987",
            "e5ef77ed69af4fdcbcac43d127665ba3",
            "1f4eb126464442248b8b1b0ae1ff6c2e",
            "b8ba481328bf4c6593e64d7d011b9a24",
            "f3c8e5e9c8534f838d22890ce4476652",
            "4ea2dcb9a7e944568ec9e46bc070cfe3",
            "1620bf2eecbe410bba632f8e272efa63",
            "8166984435f641748fe49b9fd7193f30",
            "9751a14119214e4b877d8ef6b886adfb",
            "ce38a9bf0ee54613a14305226f78f7be",
            "a795020fc5f9409e9ab8483ab69b0b85",
            "b6ce5cc5479f435d82be248f08c98dd5",
            "90f1ee8c64794a33b809b2c45e1e44d4",
            "c1a3755caad1425daf9043d9b38c6174",
            "b6c622304fe34a9993ac34ae1413b215"
          ]
        },
        "id": "ibNymkYaWFPD",
        "outputId": "9c3900e1-25a2-49fe-b2b2-1420797a6b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9bb1762c32c4b2f9a2242ba90671360"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3c8e5e9c8534f838d22890ce4476652"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coreference Clusters: [['Kiran', 'he'], ['Sarita', 'She']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict coreference resolution for your input text\n",
        "preds = model.predict(\n",
        "    texts=['Kiran is a good student, and Sita is his science teacher, and he teaches him science.']\n",
        ")\n",
        "\n",
        "# Retrieve the coreference clusters from the prediction\n",
        "clusters = preds[0].get_clusters()\n",
        "\n",
        "# Print the clusters\n",
        "print(\"Coreference Clusters:\", clusters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "8313ba53c97a4a25abe94c5405c09652",
            "ab6f36b81b334e5c83a1e7cb923090f9",
            "00028589c4af40b895753e254c6f70d4",
            "6b3b048e89e14bd8b6494ea6c4ee4a13",
            "d0184fd50d8c4d358707eeaad2f7743b",
            "bcc93be3e56b47f283ef5fe89c853379",
            "e74e2541f7d54dfc99240656bd4b2dbd",
            "234731d98abf4e7fa0f95f65fc7804b7",
            "82e18d6b1fec47c0949ddc1e03cb6c61",
            "81162184eaf740eb894765f3e722e849",
            "70be41d1860341bdb5080a240225f952",
            "c0a405da18734cdeb997b980fd863edb",
            "af37363e479f4c5699b2dcde1bbe9ece",
            "d67e604fe23849618933ed0c34a37094",
            "4bce27c7b50b40ed9fb2dafa7df7ab2b",
            "97ee367fc916450a8e007d86b34ae2a7",
            "e69abc1fd2e84957bbc4ce18e4f1021e",
            "b2107253031a46fd8dadaf8bbf21dbbd",
            "1cd9f971b2ff4414b9a1c42205df22f3",
            "2337c251463b4b86a528555a190647fa",
            "a346880f64444e928d7ca276a88c975e",
            "f58e426707c14d2aad111be189623526"
          ]
        },
        "id": "LrSBvbCwyeeQ",
        "outputId": "44944f8d-e2d9-4dcf-9eaa-12b8043498b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8313ba53c97a4a25abe94c5405c09652"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0a405da18734cdeb997b980fd863edb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coreference Clusters: [['Kiran', 'his', 'him'], ['Sita', 'he']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict coreference resolution for your input text\n",
        "preds = model.predict(\n",
        "    texts=['Kiran is a good student and he goes to school where Sarita is the teacher. She teaches maths.']\n",
        ")\n",
        "\n",
        "# Retrieve the coreference clusters from the prediction\n",
        "clusters = preds[0].get_clusters()\n",
        "\n",
        "# Print the clusters\n",
        "print(\"Coreference Clusters:\", clusters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "fb73ed2426bc4f188c69755ac67c89a0",
            "6f0e2a19fb8f476988451d7aab3cab5a",
            "e721770227884869acc03cded5f45702",
            "14a2971c38304a61b3f29f1bd420be4c",
            "0772bee775164f08b0617cdc0f9a13db",
            "c27915790b1341559f8a2028db29f478",
            "b87bcacbb8b945759b198cc091f19c45",
            "8fce692d739f405894dafea21d2d00d8",
            "23e7978af4e44ba396f27f652fd1280d",
            "502b493354da49cd8dd7a1e897ff0757",
            "3344b05f60de47fd9cc3887194bd6ba8",
            "f8c367c6972941e7b8b6c6a5bc717fa5",
            "75afa9798c324694a63555dad0a1dea7",
            "fa6a0e2c2abf4ad79206e05e91f3b75a",
            "054aba95fdbe4eab9caefe69a8ece066",
            "9f369186c97b4295868375292113e2fb",
            "00ce2756f4e44579940fbcdfef4e4433",
            "1c8f50b6148049f882422806cde02851",
            "39ca5a86631a41c183b8679f5ce5b0e7",
            "444ec30215cf422982422397f3848ebf",
            "e8b8dbcb5a264ac0b3dcb2a0494c0226",
            "a939505a68614201a58ba66351b8f846"
          ]
        },
        "id": "Dgv6aw7SZL5D",
        "outputId": "2175578b-c5fc-4fe6-f863-ed5d03549e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb73ed2426bc4f188c69755ac67c89a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8c367c6972941e7b8b6c6a5bc717fa5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coreference Clusters: [['Kiran', 'he'], ['Sarita', 'She']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict coreference resolution for your input text\n",
        "preds = model.predict(\n",
        "    texts=['Kiran is a good student and she goes to school.']\n",
        ")\n",
        "\n",
        "# Retrieve the coreference clusters from the prediction\n",
        "clusters = preds[0].get_clusters()\n",
        "\n",
        "# Print the clusters\n",
        "print(\"Coreference Clusters:\", clusters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119,
          "referenced_widgets": [
            "5ed0a95399894f3e8b565fde8b60add2",
            "e7f7ec4785a443efa19db1644f7be5ef",
            "37b25d59870345248f1ff8ee50e516fc",
            "ed738427b11a4b6f906c8a8bfe27f6b2",
            "9e1e9a7036bd468b871b005685476b2f",
            "46f3b189dedb49c6b9b8d43ef6aa8187",
            "2b3d9e0fb56045c98f2e0de813614d10",
            "40018450666c4dc7a15a2b8510737fce",
            "74f6d7ae92e74b819130a75df560634b",
            "66a4cf33e49a4f3bb1ade664e68dd7e5",
            "7bf34a552d4c4a158b19acf857e1b84c",
            "8cc2250dbbec47ae8e5e3bb0e1e8f37d",
            "3db32b73ae4a4cf39c2b1c27e272cde9",
            "a940bcba493f4610a3bba919f77da398",
            "b4f92a4dc19345c28a2a775c481cf796",
            "926fa37d96d149be8e06a0b27b36ab48",
            "ae04a28d0265413a8f77681c48f15cde",
            "4cbf9f66c779438580f613e8ecf4de80",
            "4a9a1d43cfd0483d895829bfe1276229",
            "23956859efe84dc38f84cbfa414416cf",
            "1a0f6efebbdb47c699b0ead299ffacfb",
            "e63a0553282a418db428f8d5eb4d8945"
          ]
        },
        "id": "IMDft_ZQn_oS",
        "outputId": "7bf4aada-adea-4a96-f8f2-b1bddc5906dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ed0a95399894f3e8b565fde8b60add2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cc2250dbbec47ae8e5e3bb0e1e8f37d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coreference Clusters: [['Kiran', 'she']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict coreference resolution for your input text\n",
        "preds = model.predict(\n",
        "    texts=['Kiran is a good student in a school where Alexa is his teacher. He goes to school regularly and she teaches maths.']\n",
        ")\n",
        "\n",
        "# Retrieve the coreference clusters from the prediction\n",
        "clusters = preds[0].get_clusters()\n",
        "\n",
        "# Print the clusters\n",
        "print(\"Coreference Clusters:\", clusters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "95f4723cad8b4fa79f8cc9110bef5e14",
            "80ad582e52a541c7920b524315afc364",
            "48e003f212f448a19e757f9a2c10675d",
            "bf349bbf8b3b4a23bce0fcae8690a99b",
            "0c84c746808f4fe791d26e82ac8186a8",
            "c665901d967a458ea2e3232638f86f3f",
            "fadd63a84a5643c6ad7c07e97da000b5",
            "f1de7d07df08493aa6f5573f4714cd44",
            "15e72b4e00a94b7a922a0c7679a34c29",
            "ec9a008d251b4e92a88e9ae061486c51",
            "ecc116b12ab0454cb2087e0e9308b5d1",
            "16591168c431424a992b8b50c623bdab",
            "be4cfd6ddf1a485fb4663e908a336afa",
            "b0bb6ffc2993422c92c14d8fba9570ba",
            "df5fe7b487aa4251a176cfe6c9038697",
            "8d32085f1b9847c1b0d80a1fa69e93ac",
            "1955ed821a7a489eb7109e3c5df92a26",
            "cf1c59f191ae45e99c199af28583af07",
            "c530340b221a424d93fe4119f968a640",
            "88d6dde9305549b285b61940f7984c6a",
            "cb00f3aeb97243f19b6a41b352eec590",
            "ba1b3a829be744098b2c8c88db92f40e"
          ]
        },
        "id": "494_af4HpGSt",
        "outputId": "f481a8d1-abf6-4b27-c5e2-6dcf58cbf23a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95f4723cad8b4fa79f8cc9110bef5e14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16591168c431424a992b8b50c623bdab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coreference Clusters: [['Kiran', 'his', 'He'], ['Alexa', 'she']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict coreference resolution for your input text\n",
        "preds = model.predict(\n",
        "    texts=['Kiran is a good student, he goes to school regularly. Jack is his teacher and she teaches maths.']\n",
        ")\n",
        "\n",
        "# Retrieve the coreference clusters from the prediction\n",
        "clusters = preds[0].get_clusters()\n",
        "\n",
        "# Print the clusters\n",
        "print(\"Coreference Clusters:\", clusters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "8a178518b6934c1ebb4f9b058b209c40",
            "b0ea73a21cd143d988f795de9ab368fd",
            "25152753f24f433baa174e60c4872dee",
            "7da1dd2032e14c9f9c36baae22750871",
            "e81ed5ec8ebf4d228a8e88c3fea90bb4",
            "cc2cbf3521ca40f283d52f79a0fb8d35",
            "d04d1c0da9bd45278f833b0eb1566477",
            "b0f2a46b6e29439cacf1cc81f41f1230",
            "1c54bd5920064f9fb71b8c58c0c16148",
            "682ef2e348fa424290ee0b8f1160fe5c",
            "a7bafbaa41464d7f900b0ebb7c96c949",
            "2c0412d3e2494066bdf285d316a537c7",
            "287dda027e1c4354bfff8be7c2b8836c",
            "67ad9f87814b4ad88d844fc9429a6295",
            "c189e264c9144d2882ac35716f46b970",
            "f0cec99c48d14e27a910a4431a08835c",
            "0e81577bf0b4446a9c8eb65d5721f33f",
            "2f2ec9d5c74d42539cc9c18aafe14ea1",
            "4b9917ba66ec4c729b7f62725a23b07a",
            "fd157a9ad23b452eb0e5144c1241bba0",
            "9d81acc74ba04b6ab916577cb2c8a5fc",
            "c79ad509c9324a64a4a5b333af5f292f"
          ]
        },
        "id": "pnzA5pX_2tXc",
        "outputId": "4fa61b34-6b34-4219-d430-6dd6ef477321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a178518b6934c1ebb4f9b058b209c40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Inference:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c0412d3e2494066bdf285d316a537c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coreference Clusters: [['Kiran', 'he', 'his']]\n"
          ]
        }
      ]
    }
  ]
}